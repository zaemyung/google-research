{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9GoqUwZXVqH"
      },
      "source": [
        "  Copyright 2022 Google LLC.\n",
        "\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_MfrX6pDMITb"
      },
      "outputs": [],
      "source": [
        "# @title Install required packages\n",
        "def install_required_packages():\n",
        "  global gym_minigrid\n",
        "  global mon_minigrid\n",
        "  global mdp_wrapper\n",
        "  global coloring_wrapper\n",
        "  !pip install dm-haiku\n",
        "  !pip install flax\n",
        "\n",
        "  !pip install gym_minigrid\n",
        "  !git clone https://github.com/google-research/google-research.git\n",
        "\n",
        "  GOOGLE_RESEARCH_PATH = './google-research'\n",
        "  import sys\n",
        "  if GOOGLE_RESEARCH_PATH not in sys.path:\n",
        "    sys.path.append(GOOGLE_RESEARCH_PATH)\n",
        "\n",
        "  from minigrid_basics.envs import mon_minigrid\n",
        "  from minigrid_basics.custom_wrappers import mdp_wrapper\n",
        "  from minigrid_basics.custom_wrappers import coloring_wrapper\n",
        "\n",
        "install_required_packages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mxnXaRSeNZIy"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.scipy as jsp\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import scipy as sp\n",
        "import scipy.linalg\n",
        "import matplotlib.pyplot as plt\n",
        "import gin\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pdb\n",
        "import sklearn\n",
        "import tensorflow.compat.v1 as tf\n",
        "import gym\n",
        "import pdb\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from jax import random\n",
        "from jax import grad, jit\n",
        "from flax import linen as nn\n",
        "import collections\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import functools\n",
        "import time\n",
        "import pdb\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xyZftUUKTEfv"
      },
      "outputs": [],
      "source": [
        "# @title Losses\n",
        "\n",
        "def inner_objective_td(Phi, G, gamma, P, W):\n",
        "  return jnp.sum(jnp.square(Phi @ W - G - jax.lax.stop_gradient(gamma * P @ Phi @ W)))\n",
        "\n",
        "def outer_objective_td(Phi, G, gamma, P):\n",
        "  # Outer Objective function: $ \\min_W \\|\\Phi W - sg(G - gamma P Phi W) \\|^2_F$\n",
        "  W_star, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ G)\n",
        "  return inner_objective_td(Phi, G, gamma, P, W_star)\n",
        "  \n",
        "def mspbe(Phi, G, gamma, P, W):\n",
        "  return jnp.linalg.norm(Phi @ W - Phi @ jnp.linalg.solve(Phi.T @ Phi, jnp.eye(Phi.shape[1])) @ Phi.T @ (G + gamma * P @ Phi @ W), ord='fro')**2\n",
        "\n",
        "def outer_mspbe(Phi, G, gamma, P):\n",
        "  W, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ G)\n",
        "  return jnp.linalg.norm(Phi @ W - Phi @ jnp.linalg.solve(Phi.T @ Phi, jnp.eye(Phi.shape[1])) @ Phi.T @ (G + gamma * P @ Phi @ W), ord='fro')**2\n",
        "\n",
        "def mspbe_nonlinear(params, Phi, G, gamma, P, W):\n",
        "  return jnp.linalg.norm(params @ jnp.linalg.solve(params.T @ params, jnp.eye(params.shape[1])) @ params.T @ (G + gamma * P @ Phi @ W - Phi @ W), ord='fro')**2\n",
        "\n",
        "def msbe_phi(Phi, G, gamma, P, W):\n",
        "  return jnp.linalg.norm(Phi @ W - G @ W.T @ jnp.linalg.solve(W @ W.T, jnp.eye(Phi.shape[1])) @ W - gamma * P @ Phi @ W, ord='fro')**2\n",
        "\n",
        "def outer_mspbe_Phi(Phi, G, gamma, P):\n",
        "   W, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ G)\n",
        "   return jnp.linalg.norm(Phi @ W - G @ W.T @ jnp.linalg.solve(W @ W.T, jnp.eye(Phi.shape[1])) @ W - gamma * P @ Phi @ W, ord='fro')**2\n",
        "  \n",
        "\n",
        "def outer_objective_mspbe(Phi, G, gamma, P, W):\n",
        "  W_star, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ G)\n",
        "  return inner_objective_td_true(Phi, G, gamma, P, W_star)\n",
        "\n",
        "def policy_evaluation_error_td(Phi, G, gamma, P):\n",
        "  W_star, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ G)\n",
        "  return jnp.linalg.norm(Phi @ W_star - G - gamma * P @ Phi @ W_star, ord='fro')**2\n",
        "\n",
        "def inner_objective_res(Phi, G, gamma, P, W):\n",
        "  # Inner Objective function: $\\|\\Phi W - (G - gamma P Phi W) \\|^2_F$\n",
        "  return jnp.linalg.norm(Phi @ W - G - gamma * P @ Phi @ W, ord='fro')**2\n",
        "\n",
        "def outer_objective_res(Phi, G, gamma, P):\n",
        "  # Outer Objective function: $ \\min_W \\|\\Phi W - (G - gamma P Phi W) \\|^2_F$\n",
        "  W_star, _, _, _ = jnp.linalg.lstsq((Phi - gamma * P @ Phi), G)\n",
        "  return inner_objective_res(Phi, G, gamma, P, W_star)\n",
        "\n",
        "def inner_objective_mc(Phi, Psi, W):\n",
        "  # Inner Objective function: $\\|\\Phi W - \\Psi \\|^2_F$\n",
        "  return jnp.linalg.norm(Phi @ W - Psi, ord='fro')**2\n",
        "\n",
        "def outer_objective_mc(Phi, Psi):\n",
        "  # Outer objective function: $J(\\Phi) =\\min_W \\|\\Phi W - \\Psi \\|^2_F$\n",
        "  W_star, _, _, _ = jnp.linalg.lstsq(Phi, Psi, rcond=1e-5)\n",
        "  return inner_objective_mc(Phi, Psi, W_star)\n",
        "\n",
        "def grassman_distance(Y1, Y2):\n",
        "  Q1, _ = jnp.linalg.qr(Y1)\n",
        "  Q2, _ = jnp.linalg.qr(Y2)\n",
        "  _, sigma, _ = jnp.linalg.svd(Q1.T @ Q2)\n",
        "  # sigma = jnp.clip(sigma, -1., 1.)\n",
        "  sigma = jnp.round(sigma, decimals=5)\n",
        "  # pdb.set_trace()\n",
        "  return jnp.linalg.norm(jnp.arccos(sigma))\n",
        "\n",
        "\n",
        "grad_Phi_td = jax.grad(inner_objective_td, argnums=0)\n",
        "grad_W_td = jax.grad(inner_objective_td, argnums=4)\n",
        "grad_W_Phi_td = jax.grad(inner_objective_td, argnums=(0, 4))\n",
        "\n",
        "grad_Phi_res = jax.grad(inner_objective_res, argnums=0)\n",
        "grad_W_res = jax.grad(inner_objective_res, argnums=4)\n",
        "grad_W_Phi_res = jax.grad(inner_objective_res, argnums=(0, 4))\n",
        "\n",
        "grad_Phi_mc = jax.grad(inner_objective_mc, argnums=0) # shape (S, d)\n",
        "grad_W_mc = jax.grad(inner_objective_mc, argnums=2) # shape (d, T)\n",
        "grad_W_Phi_mc = jax.grad(inner_objective_mc, argnums=(0, 2)) # tuple shape (S, d) and (d, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wavaF1D1XyaW"
      },
      "outputs": [],
      "source": [
        "S = 2\n",
        "T = S\n",
        "d = 2\n",
        "\n",
        "from jax.config import config\n",
        "config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "gamma = 0.99\n",
        "Phi = random.normal(random.PRNGKey(0), (S, d), dtype=jnp.float64)\n",
        "Phi = Phi / jnp.linalg.norm(Phi)\n",
        "W = random.normal(random.PRNGKey(10), (d, 1), dtype=jnp.float64)\n",
        "W = W / jnp.linalg.norm(W)\n",
        "W_tile = jnp.tile(W, (1, S))\n",
        "params = jnp.concatenate((W_tile.T, Phi), axis=1)\n",
        "\n",
        "G = random.normal(random.PRNGKey(0), (S, T), dtype=jnp.float64)\n",
        "P = jax.random.normal(random.PRNGKey(0), (S, S), dtype=jnp.float64) #+ jax.random.normal(random.PRNGKey(0), (S, S), dtype=jnp.float64).T\n",
        "P /= jnp.sum(P, axis=1)\n",
        "\n",
        "W_star_td = sp.linalg.solve(Phi.T @ (Phi - gamma * P @ jax.lax.stop_gradient(Phi)), jnp.eye(d)) @ Phi.T @ G\n",
        "mspbe(Phi, G, gamma, P, W_star_td)\n",
        "jnp.linalg.solve(params.T @ params, jnp.eye(params.shape[1]))\n",
        "print(params @ jnp.linalg.solve(params.T @ params, jnp.eye(2*d)) @ params.T)\n",
        "P_perp = (jnp.eye(S) - W_tile.T @ (jnp.linalg.solve(W_tile @ W_tile.T, jnp.eye(d)))@W_tile)\n",
        "print(P_perp @ Phi @ jnp.linalg.solve(Phi.T @ P_perp @ Phi, jnp.eye(d)) @ Phi.T @ (-P_perp) + jnp.eye(S))\n",
        "jax.grad(mspbe_nonlinear, argnums=(0))(params, Phi, G, gamma, P, W), jax.grad(inner_objective_td, argnums=(0, 4))(Phi, G, gamma, P, W)\n",
        "A = W_tile @ W_tile.T\n",
        "B = W_tile @ Phi\n",
        "C = Phi.T @ W_tile.T\n",
        "D = Phi.T @ Phi\n",
        "A_inv = jnp.linalg.solve(A, jnp.eye(d))\n",
        "inv_term = jnp.linalg.solve(D - C @ A_inv @ B, jnp.eye(d))\n",
        "M_D = A = B@ jnp.linalg.solve(D, jnp.eye(d)) @ C\n",
        "M_D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFWUCD9tq-Fc"
      },
      "outputs": [],
      "source": [
        "print(jnp.linalg.solve(params.T @ params, jnp.eye(params.shape[1])))\n",
        "jnp.linalg.matrix_rank(W_tile), jnp.linalg.matrix_rank(Phi)\n",
        "# jnp.linalg.solve(params.T @ params, jnp.eye(2*d))\n",
        "W @ W.T, Phi.T @ Phi, params.T @ params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlZH8KVyY3wK"
      },
      "outputs": [],
      "source": [
        "W = random.normal(random.PRNGKey(0), (d, T), dtype=jnp.float64) * 0\n",
        "Phi_star = random.normal(random.PRNGKey(0), (S, d), dtype=jnp.float64) * 0 #jnp.linalg.solve(jnp.eye(S) - gamma * P, jnp.eye(S)) @ G @ W.T @ jnp.linalg.solve(W @ W.T, jnp.eye(d))\n",
        "msbe_phi(Phi_star, G, gamma, P, W)\n",
        "# jnp.linalg.norm((Phi_star @ W).T - W.T @ jnp.linalg.solve(W @ W.T, jnp.eye(Phi_star.shape[1])) @ W @ G.T - gamma * (Phi_star @ W).T @ P.T, ord='fro')**2\n",
        "msbe_phi(Phi_star, G, gamma, P, W)\n",
        "# msbe_phi(jnp.linalg.solve(jnp.eye(S) - gamma * P, jnp.eye(S)) @ G / W, G, gamma, P, W)\n",
        "jnp.linalg.norm(Phi_star @ W @ W.T - gamma * P @ Phi_star @ W @ W.T - G @ W.T, ord='fro')**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQwliG-XsBIN"
      },
      "outputs": [],
      "source": [
        "def f(Phi, W, i):\n",
        "  return (Phi @ W)[i][0]\n",
        "\n",
        "f(Phi, W, 0)\n",
        "params = (Phi, W)\n",
        "jax.grad(f, argnums=(0, 1))(Phi, W, 0) , W, Phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z72OYwY2TpMa"
      },
      "outputs": [],
      "source": [
        "# @title RL basics\n",
        "def policy_random(env):\n",
        "  \"\"\"Random policy on env.\n",
        "\n",
        "  Args:\n",
        "    env: a MiniGrid environment, including the MDPWrapper.\n",
        "\n",
        "  Returns:\n",
        "    Numpy array S \\times A: random policy\n",
        "  \"\"\"\n",
        "  return np.ones((env.num_states, env.num_actions)) / env.num_actions\n",
        "\n",
        "def trans_matrix(env, policy):\n",
        "  trans_matrix = np.zeros((env.num_states, env.num_states))\n",
        "  # The transition probability is S x A x S. We need to make it S x S here.\n",
        "  for s in range(env.num_states):\n",
        "    for a in range(env.num_actions):\n",
        "      for s_next in range(env.num_states):\n",
        "        trans_matrix[s][s_next] += env.transition_probs[s][a][s_next] * policy[s][a]\n",
        "  return trans_matrix\n",
        "\n",
        "def get_state_xy(idx, num_cols):\n",
        "  \"\"\"\n",
        "  Given the index that uniquely identifies each state this method returns its equivalent coordinate (x,y).\n",
        "\n",
        "  :param idx: index uniquely identifying a state\n",
        "  :return: values x, y describing the state's location in the grid\n",
        "  \"\"\"\n",
        "  y = int(idx % num_cols)\n",
        "  x = int((idx - y) / num_cols)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def get_state_idx(x, y, num_cols):\n",
        "    \"\"\"\n",
        "    Given a state coordinate (x,y) this method returns the index that uniquely identifies this state.\n",
        "\n",
        "    :param x: value of the coordinate x\n",
        "    :param y: value of the coordinate y\n",
        "    :return : unique index identifying a position in the grid\n",
        "    \"\"\"\n",
        "    idx = y + x * num_cols\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gz45Bw27ODa5"
      },
      "outputs": [],
      "source": [
        "# @title Various MDPs\n",
        "\n",
        "# Disconnected MDP\n",
        "def P_disconnected(n):\n",
        "  return np.eye(n)\n",
        "\n",
        "# Baird's MDP aka \"waterfall MDP\"\n",
        "def P_star_terminal(n):\n",
        "  mat = np.zeros((n, n))\n",
        "  mat[:, -1] = 1\n",
        "  return mat\n",
        "\n",
        "# Baird's MDP aka \"waterfall MDP\" with non terminal state n\n",
        "def P_star_non_terminal(n):\n",
        "  mat = np.zeros((n, n))\n",
        "  mat[:n-1, -1] = 1\n",
        "  mat[n-1, :n-1] = 1 / (n-1)\n",
        "  return mat\n",
        "\n",
        "# Random walk on discrete torus MDP = Cycle (mentioned in https://arxiv.org/pdf/2101.07123.pdf)\n",
        "def P_torus(n):\n",
        "  assert n \u003e= 3\n",
        "  c = np.zeros((n,))\n",
        "  c[1] = 0.5\n",
        "  c[-1] = 0.5\n",
        "  return sp.linalg.circulant(c)\n",
        "\n",
        "def circle(n):\n",
        "  assert n \u003e= 3\n",
        "  c = np.zeros((n,))\n",
        "  c[-1] = 1.\n",
        "  return sp.linalg.circulant(c)  \n",
        "\n",
        "# Fully connected MDP\n",
        "def P_fullyconnected(n):\n",
        "  assert n \u003e= 2\n",
        "  return (np.ones((n, n)) - np.eye(n)) / (n - 1)\n",
        "\n",
        "# Linear Chain\n",
        "def P_chain(n):\n",
        "  P = P_torus(n)\n",
        "  P[0, 0] = 0.5\n",
        "  P[0, n-1] = 0.\n",
        "  P[n-1, 0] = 0.\n",
        "  P[n-1, n-1] = 0.5   \n",
        "  return P\n",
        "\n",
        "# Lattice MDP: square grid w/ x-coordinates being in the range 1, ..., 10 and y-coordinates being in the range 1, ..., 10\n",
        "lattice = mon_minigrid.MonMiniGridEnv(\n",
        "    ascii_grid= \"\"\"\n",
        "**********************\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "*                    *\n",
        "**********************\"\"\",\n",
        "directional = True,\n",
        "agent_pos = None,\n",
        "goal_pos = None)\n",
        "lattice = mdp_wrapper.MDPWrapper(lattice)\n",
        "\n",
        "P_lattice = trans_matrix(lattice, policy_random(lattice))\n",
        "\n",
        "small_lattice = mon_minigrid.MonMiniGridEnv(\n",
        "    ascii_grid= \"\"\"\n",
        "************\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "*          *\n",
        "************\"\"\",\n",
        "directional = True,\n",
        "agent_pos = None,\n",
        "goal_pos = None)\n",
        "small_lattice = mdp_wrapper.MDPWrapper(small_lattice)\n",
        "\n",
        "P_small_lattice = trans_matrix(small_lattice, policy_random(lattice))\n",
        "\n",
        "# 2d cycle\n",
        "# for every (i, j) the neighbors are just (i + 1 mod n, j), (i - 1 mod n, j), (i, j+1 mod n), (i, j-1 mod n)\n",
        "# x-coordinates being in the range 1, ..., n and y-coordinates being in the range 1, ..., n\n",
        "def P_torus_2d(num_states):\n",
        "  n = int(np.sqrt(num_states))\n",
        "  mat = np.zeros((num_states, num_states))\n",
        "  for idx in range(num_states):\n",
        "    x, y = get_state_xy(idx, n)\n",
        "    nn1 = get_state_idx((x+1)%n, y, n)\n",
        "    nn2 = get_state_idx((x-1)%n, y, n)\n",
        "    nn3 = get_state_idx(x, (y+1)%n, n)\n",
        "    nn4 = get_state_idx(x, (y-1)%n, n)\n",
        "    mat[idx, nn1] = 0.25\n",
        "    mat[idx, nn2] = 0.25\n",
        "    mat[idx, nn3] = 0.25\n",
        "    mat[idx, nn4] = 0.25\n",
        "  return mat\n",
        "\n",
        "def assert_square(F):\n",
        "  assert len(F.shape) == 2\n",
        "  assert F.shape[0] == F.shape[1]\n",
        "\n",
        "def assert_transition_matrix(P):\n",
        "  assert_square(P)\n",
        "  assert (P \u003e= 0).all()\n",
        "  assert (P \u003c= 1).all()\n",
        "  assert np.allclose(P.sum(axis=1), np.ones((P.shape[0],)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KioSZyCy0D9G"
      },
      "outputs": [],
      "source": [
        "def approx_error(Phi, v):\n",
        "  S, _ = Phi.shape\n",
        "  k = np.linalg.matrix_rank(Phi)\n",
        "  if test_orthogonal(Phi):\n",
        "    F_k = Phi\n",
        "    P_perp_term = (np.eye(S) - F_k@F_k.T) @ v\n",
        "    approx_error = jnp.linalg.norm(P_perp_term, ord='fro')**2\n",
        "    return approx_error\n",
        "  else:\n",
        "    F, _, _ = jnp.linalg.svd(Phi)\n",
        "    F_k = F[:, :k]\n",
        "    P_perp_term = (np.eye(S) - F_k@F_k.T) @ v\n",
        "    approx_error = jnp.linalg.norm(P_perp_term, ord='fro')**2\n",
        "    return approx_error\n",
        "\n",
        "def test_orthogonal(F):\n",
        "  return np.allclose(F.T @ F, np.eye(F.shape[1]), atol=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhQHaNu4xYGz"
      },
      "outputs": [],
      "source": [
        "def lstd_solution(Phi, P, nu, r, gamma):\n",
        "  # infinite sample lastd solution\n",
        "  N = nu * np.eye(S)\n",
        "  right = Phi.T @ N @ r\n",
        "  left = Phi.T @ N @ (np.eye(S) - gamma * P) @ Phi \n",
        "  x, _, _, _ = np.linalg.lstsq(left, right, rcond=1e-5)\n",
        "  return x\n",
        "\n",
        "\n",
        "def bellman_variance(P, r, Phi, gamma, nu, num_iterations=1000):\n",
        "  # empirical estimation of the variance of the bellman residual\n",
        "  # nu is the stationary distribution\n",
        "  S, d = Phi.shape\n",
        "  L = []\n",
        "  N = nu * np.eye(S)\n",
        "  right = Phi.T @ N @ r\n",
        "  left = Phi.T @ N @ (np.eye(S) - gamma * P)\n",
        "  # pdb.set_trace()\n",
        "  V, _, _, _ = np.linalg.lstsq(left, right, rcond=1e-5)\n",
        "  for i in range (num_iterations):\n",
        "    # s is drawn according to the stationary distribution\n",
        "    u = np.random.rand(1, P.shape[0])\n",
        "    cdf_s = np.cumsum(nu)\n",
        "    s = (u \u003c cdf_s).argmax()\n",
        "    # s' is drawn according to the distribution incuced by P over the states\n",
        "    u_prime = np.random.rand(1, P.shape[0])\n",
        "    cdf_s_prime = np.cumsum(P[s])\n",
        "    s_prime = (u_prime \u003c cdf_s).argmax()\n",
        "    # pdb.set_trace()\n",
        "    L.append((V[s] - r[s] - gamma * V[s_prime])**2)\n",
        "  return sum(L) / len(L)\n",
        "\n",
        "def stationary_distribution(P):\n",
        "  vals , v = np.linalg.eig(P.T)\n",
        "  vals = np.round(vals, decimals=6)\n",
        "  idx = np.where(np.round(vals, decimals=6) == 1.)[0][0]\n",
        "  return v[:, idx] / np.sum(v[:, idx])\n",
        "\n",
        "def rho_0(Phi, dis):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    Phi: S times d matrix\n",
        "  \"\"\"\n",
        "  S, d = Phi.shape\n",
        "  N = dis * np.eye(S)\n",
        "  Xi = Phi.T @ N @ Phi\n",
        "  return np.max(np.sum(np.square(Phi @ np.linalg.inv(scipy.linalg.sqrtm(Xi))), axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAbnVCh1OTUE"
      },
      "outputs": [],
      "source": [
        "S = 123\n",
        "gamma = 0.99\n",
        "P = P_torus(S)\n",
        "assert_transition_matrix(P)\n",
        "Phi = sp.linalg.solve(np.eye(S) - gamma * P, np.eye(S))\n",
        "#have stationary distributions: P star terminal, disconnected, torus, fully connected, chain, lattice, 2dtorus\n",
        "nu = stationary_distribution(P)\n",
        "r = np.ones(S)\n",
        "Phi @ lstd_solution(Phi, P, nu, r, gamma), Phi @ r\n",
        "bellman_variance(P, r, Phi, gamma, nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Afs3F3WQ6oT"
      },
      "outputs": [],
      "source": [
        "np.round((P**100)[0], decimals=6) == np.round((P**100)[0] @ P, decimals=6)\n",
        "# lamb, v = np.linalg.eig(P.T)\n",
        "# (0.45 \u003c np.cumsum(stationary_distribution(P)))\n",
        "# np.round(P.T @ v[:, 2], decimals=6) == np.round(v[:, 2], decimals=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhvOFLgU2kZN"
      },
      "outputs": [],
      "source": [
        "l = {'Disconnected': P_disconnected, 'Chain': P_chain, 'Fully connected': P_fullyconnected, 'Star': P_star_terminal}\n",
        "fig, axes = plt.subplots(ncols=4, nrows=2, figsize=(5*4, 8))\n",
        "i = 0\n",
        "for (key, p) in l.items():\n",
        "  print(key)\n",
        "  ax = axes[:, i]\n",
        "  i +=1\n",
        "  s_range = range(3, 400)\n",
        "  var_range_all_ones = []\n",
        "  var_range_one_hot = []\n",
        "  var_range_gauss = []\n",
        "  for S in s_range:\n",
        "    P = p(S)\n",
        "    assert_transition_matrix(P)\n",
        "    Phi = sp.linalg.solve(np.eye(S) - gamma * P, np.eye(S))\n",
        "    r_all_ones = np.ones((S,))\n",
        "    r_one_hot = np.zeros((S,))\n",
        "    r_one_hot[-1] = 1.0\n",
        "    r_gauss = np.random.normal(size=S)\n",
        "    r_gauss /= np.linalg.norm(r_gauss, ord=np.inf) # normalize to R_max=1\n",
        "\n",
        "    nu = stationary_distribution(P)\n",
        "    var_range_all_ones.append(bellman_variance(P, r_all_ones, Phi, gamma, nu))\n",
        "    var_range_one_hot.append(bellman_variance(P, r_one_hot, Phi, gamma, nu))\n",
        "    var_range_gauss.append(bellman_variance(P, r_gauss, Phi, gamma, nu))\n",
        "\n",
        "  ax[0].plot(s_range, var_range_all_ones)\n",
        "  ax[1].plot(s_range, var_range_one_hot)\n",
        "  # plt.plot(s_range, var_range_gauss)\n",
        "  ax[0].legend(['all_ones'])\n",
        "  ax[1].legend(['one_hot'])\n",
        "  ax[0].set_title(key, size='x-large')\n",
        "  ax[0].set_xlabel('Number of states', size='x-large') \n",
        "  ax[1].set_ylabel('Bellman residual variance', size='x-large')\n",
        "  ax[1].set_xlabel('Number of states', size='x-large') \n",
        "  ax[0].set_ylabel('Bellman residual variance', size='x-large')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIOKmKRssmcK"
      },
      "outputs": [],
      "source": [
        "S = 400\n",
        "gamma = 0.99\n",
        "l = {'Chain': P_chain, 'Fully connected': P_fullyconnected, 'Star': P_star_terminal, 'torus': P_torus, '2dtorus': P_torus_2d, 'Lattice': P_lattice}\n",
        "\n",
        "fig, axes = plt.subplots(ncols=6, nrows=1, figsize=(5*6, 4))\n",
        "i = 0\n",
        "for (key, p) in l.items():\n",
        "  if isinstance(p, np.ndarray):\n",
        "    pass\n",
        "  else:\n",
        "    P = p(S)\n",
        "  print(key)\n",
        "  # ax = axes[:, i]\n",
        "  ax = axes[i]\n",
        "  i +=1\n",
        "  k_range = range(3, S)\n",
        "  approx_range_sl = []\n",
        "  approx_range_res = []\n",
        "  approx_range_td = []\n",
        "  _, Z = scipy.linalg.schur(P)\n",
        "  assert_transition_matrix(P)\n",
        "  Psi = sp.linalg.solve(np.eye(S) - gamma * P, np.eye(S))\n",
        "  F, s, B = np.linalg.svd(Psi)\n",
        "  G = jnp.eye(S)\n",
        "  G_S, sigma_g, _ = jnp.linalg.svd(G)\n",
        "  r_all_ones = np.ones((S,))\n",
        "  r_one_hot = np.zeros((S,))\n",
        "  r_one_hot[-1] = 1.0\n",
        "  r_gauss = np.random.normal(size=S)\n",
        "  r_gauss /= np.linalg.norm(r_gauss, ord=np.inf) # normalize to R_max=1\n",
        "  v_all_ones = (Psi @ r_all_ones).reshape(S, 1)\n",
        "  v_one_hot = (Psi @ r_one_hot).reshape(S, 1)\n",
        "  v_gauss = (Psi @ r_gauss).reshape(S, 1)\n",
        "\n",
        "  for k in k_range:\n",
        "    approx_range_sl.append(approx_error(F[:, :k], v_one_hot))\n",
        "    approx_range_res.append(approx_error(Psi @ G_S[:, :k], v_one_hot))\n",
        "    approx_range_td.append(approx_error(Z[:, :k], v_one_hot))\n",
        "\n",
        "  ax.plot(k_range, approx_range_sl)\n",
        "  ax.plot(k_range, approx_range_res)\n",
        "  ax.plot(k_range, approx_range_td)\n",
        "  ax.legend(['sl' , 'res', 'td'])\n",
        "  ax.set_title(key, size='x-large')\n",
        "  ax.set_xlabel('Number of features', size='x-large') \n",
        "  ax.set_ylabel('Approx error', size='x-large')\n",
        "plt.savefig('all_ones_reward.png')\n",
        "%download_file all_ones_reward.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2nc9ac1OXmU"
      },
      "outputs": [],
      "source": [
        "# @title Learning different representations with SR auxiliary tasks \n",
        "gamma = 0.99\n",
        "num_epochs = 1000\n",
        "decay = 0.\n",
        "start_learning_rate = 1e-4\n",
        "optimizer = optax.sgd(start_learning_rate)\n",
        "P = circle(9)\n",
        "S = P.shape[0]\n",
        "L = (np.eye(S) - gamma* P)\n",
        "L_inv = np.linalg.solve(L, np.eye(S))\n",
        "\n",
        "# G = jnp.eye(S, dtype=jnp.float64)\n",
        "G = jnp.zeros(S).reshape(S, 1)\n",
        "G = G.at[0].set(1)\n",
        "\n",
        "T = S\n",
        "d = 4\n",
        "\n",
        "def train_td_representation(P, num_epochs, start_learning_rate, d):\n",
        "  Phi_init, W_init = random.normal(random.PRNGKey(0), (S, d)), random.normal(random.PRNGKey(0), (d, S))\n",
        "  params = (Phi_init, W_init)\n",
        "  opt_state = optimizer.init(params)\n",
        "  inner_objective_td(params[0], G, gamma, P, params[1])\n",
        "  L = []\n",
        "  for i in range(num_epochs):\n",
        "    # alpha = 1 / (1 + decay * i) \n",
        "    grads = grad_W_Phi_td(params[0], G, gamma, P, params[1])\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    # updates = (alpha * updates[0], alpha * updates[1])\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    L.append(params)\n",
        "  return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG6TdX17uyeL"
      },
      "outputs": [],
      "source": [
        "# @title TD learning w/ outer objective\n",
        "gamma = 0.99\n",
        "num_epochs = 5000\n",
        "decay = 0.\n",
        "start_learning_rate = 1e-3\n",
        "optimizer = optax.sgd(start_learning_rate)\n",
        "P = circle(9)\n",
        "S = P.shape[0]\n",
        "\n",
        "L = (np.eye(S) - gamma* P)\n",
        "L_inv = np.linalg.solve(L, np.eye(S))\n",
        "\n",
        "# G = jnp.eye(S, dtype=jnp.float64)\n",
        "G = jnp.zeros(S).reshape(S, 1)\n",
        "G = G.at[0].set(1)\n",
        "\n",
        "T = S\n",
        "d = 2\n",
        "\n",
        "def train_td_representation_outer(P, num_epochs, start_learning_rate, d):\n",
        "  Phi_init = random.normal(random.PRNGKey(0), (S, d))\n",
        "  params = Phi_init\n",
        "  opt_state = optimizer.init(params)\n",
        "  outer_objective_td(params, G, gamma, P)\n",
        "  L = []\n",
        "  for i in range(num_epochs):\n",
        "    # alpha = 1 / (1 + decay * i) \n",
        "    alpha =1\n",
        "    grads = jax.grad(outer_objective_td, argnums=0)(params, G, gamma, P)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    # updates = (alpha * updates[0], alpha * updates[1])\n",
        "    params = optax.apply_updates(params, alpha * updates)\n",
        "    L.append(params)\n",
        "  return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0keyMXQPgakf"
      },
      "outputs": [],
      "source": [
        "L = train_td_representation_outer(P, num_epochs, start_learning_rate, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE2L38iP2cgN"
      },
      "outputs": [],
      "source": [
        "y = [outer_objective_td(k, G, gamma, P) for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "data = np.zeros(num_epochs)\n",
        "plt.plot(np.arange(1, num_epochs+1), data)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "plt.ylim(bottom=0)\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMMJK2F8Egjx"
      },
      "outputs": [],
      "source": [
        "y = [(k[:, 1].T @ (L_inv @ G) / (np.linalg.norm(k[:, 1]) * np.linalg.norm(L_inv @ G)))[0] for k in L]\n",
        "plt.plot(np.arange(1, num_epochs+1), y)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGLknRbVHUgT"
      },
      "outputs": [],
      "source": [
        "y = [max(abs((k / np.linalg.norm(k)) - ((L_inv @ G) / (np.linalg.norm(L_inv @ G))))) for k in L]\n",
        "plt.plot(np.arange(1, num_epochs+1), y)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Guaz702I0Re"
      },
      "outputs": [],
      "source": [
        "min((L[-1] / np.linalg.norm(L[-1])) - ((L_inv @ G) / (np.linalg.norm(L_inv @ G))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7GqtKTfTSIm"
      },
      "outputs": [],
      "source": [
        "y = [inner_objective_td(k[0], G, gamma, P, k[1]) for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "data = np.zeros(num_epochs)\n",
        "plt.plot(np.arange(1, num_epochs+1), data)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnVzDIS6HzxR"
      },
      "outputs": [],
      "source": [
        "y = [mspbe(k[0], G, gamma, P, k[1]) for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNGI3E-O4ux6"
      },
      "outputs": [],
      "source": [
        "y = [outer_mspbe(k, G, gamma, P) for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARNusOjSKM1y"
      },
      "outputs": [],
      "source": [
        "y = [msbe_phi(k[0], G, gamma, P, k[1]) for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4M8N4LJQ4av"
      },
      "outputs": [],
      "source": [
        "(L[-1][0] @ L[-1][1]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqf7lEZh2p9s"
      },
      "outputs": [],
      "source": [
        "y = [np.linalg.norm(k[0] @ k[1] - L_inv @ G, ord='frob')  for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('frob norm')\n",
        "plt.title('S={}, T={}, d={}, lr={}, decay={}'.format(S, T, d, start_learning_rate, decay))\n",
        "plt.savefig('image.png')\n",
        "%download_file image.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg7oo9n9Hc4t"
      },
      "outputs": [],
      "source": [
        "ranks = [np.linalg.matrix_rank(k) for k in L]\n",
        "plt.plot(np.arange(1, num_epochs+1), ranks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bWdnb8A6DQ3"
      },
      "outputs": [],
      "source": [
        "jnp.linalg.solve(L[-1].T @ L[-1], jnp.eye(d) ), jnp.linalg.matrix_rank(L[-1].T @ L[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6wHLsELTkFV"
      },
      "outputs": [],
      "source": [
        "idx = -1\n",
        "Phi = L[idx][0]\n",
        "W = L[idx][1]\n",
        "U, s, V = jnp.linalg.svd(jnp.linalg.inv(jnp.eye(S) - gamma* P))\n",
        "Phi, (Phi.T @ (jnp.eye(S) - gamma * P) @ Phi), U, V\n",
        "grad_W_Phi_td(Phi, G, gamma, P, W) # zero\n",
        "# U @ Phi, U.T @ Phi, V.T @ Phi, V @ Phi\n",
        "y = [grassman_distance(k[0], U[:, :d]) for k in L]\n",
        "plt.semilogy(np.arange(1, num_epochs+1), y)\n",
        "plt.ylim(bottom=0.0)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('grassman distance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_FRNUjVWivx"
      },
      "outputs": [],
      "source": [
        "Psi = sp.linalg.solve(np.eye(S) - gamma * P, np.eye(S))\n",
        "F, s, B = np.linalg.svd(Psi)\n",
        "r_all_ones = np.ones((S,))\n",
        "r_one_hot = np.zeros((S,))\n",
        "r_one_hot[-1] = 1.0\n",
        "r_gauss = np.random.normal(size=S)\n",
        "r_gauss /= np.linalg.norm(r_gauss, ord=np.inf) # normalize to R_max=1\n",
        "v_all_ones = (Psi @ r_all_ones).reshape(S, 1)\n",
        "v_one_hot = (Psi @ r_one_hot).reshape(S, 1)\n",
        "v_gauss = (Psi @ r_gauss).reshape(S, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfzQ0_TgW34j"
      },
      "outputs": [],
      "source": [
        "# TD representations\n",
        "Phi = L[-1][0]\n",
        "w_star_gauss, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ r_gauss)\n",
        "w_star_all_ones, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ r_all_ones)\n",
        "w_star_one_hot, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ r_one_hot)\n",
        "jnp.sum((Phi @ w_star_gauss- v_gauss)**2), jnp.sum((Phi @ w_star_all_ones- v_all_ones)**2), jnp.sum((Phi @ w_star_one_hot- v_one_hot)**2)\n",
        "max(jnp.sum((Phi @ w_star_gauss- v_gauss)**2), jnp.sum((Phi @ w_star_all_ones- v_all_ones)**2), jnp.sum((Phi @ w_star_one_hot- v_one_hot)**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkYxKtAQZ9En"
      },
      "outputs": [],
      "source": [
        "# Mc representations\n",
        "Phi = F[:, :d]\n",
        "w_star_gauss, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ r_gauss)\n",
        "w_star_all_ones, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ r_all_ones)\n",
        "w_star_one_hot, _, _, _ = jnp.linalg.lstsq(Phi.T @ (Phi - gamma * P @ Phi), Phi.T @ r_one_hot)\n",
        "jnp.sum((Phi @ w_star_gauss- v_gauss)**2), jnp.sum((Phi @ w_star_all_ones- v_all_ones)**2), jnp.sum((Phi @ w_star_one_hot- v_one_hot)**2)\n",
        "max(jnp.sum((Phi @ w_star_gauss- v_gauss)**2), jnp.sum((Phi @ w_star_all_ones- v_all_ones)**2), jnp.sum((Phi @ w_star_one_hot- v_one_hot)**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srci-jyjWVVz"
      },
      "outputs": [],
      "source": [
        "S = 400\n",
        "gamma = 0.99\n",
        "# l = {'Chain': P_chain, 'Fully connected': P_fullyconnected, 'Star': P_star_terminal, 'torus': P_torus, '2dtorus': P_torus_2d, 'Lattice': P_lattice}\n",
        "l = {'Lattice': P_lattice}\n",
        "fig, axes = plt.subplots(ncols=6, nrows=1, figsize=(5*6, 4))\n",
        "i = 0\n",
        "for (key, p) in l.items():\n",
        "  if isinstance(p, np.ndarray):\n",
        "    pass\n",
        "  else:\n",
        "    P = p(S)\n",
        "  print(key)\n",
        "  # ax = axes[:, i]\n",
        "  ax = axes[i]\n",
        "  i +=1\n",
        "  k_range = range(3, S, 50)\n",
        "  approx_range_sl = []\n",
        "  approx_range_td = []\n",
        "  assert_transition_matrix(P)\n",
        "  Psi = sp.linalg.solve(np.eye(S) - gamma * P, np.eye(S))\n",
        "  F, s, B = np.linalg.svd(Psi)\n",
        "  G = jnp.eye(S)\n",
        "  r_all_ones = np.ones((S,))\n",
        "  r_one_hot = np.zeros((S,))\n",
        "  r_one_hot[-1] = 1.0\n",
        "  r_gauss = np.random.normal(size=S)\n",
        "  r_gauss /= np.linalg.norm(r_gauss, ord=np.inf) # normalize to R_max=1\n",
        "  v_all_ones = (Psi @ r_all_ones).reshape(S, 1)\n",
        "  v_one_hot = (Psi @ r_one_hot).reshape(S, 1)\n",
        "  v_gauss = (Psi @ r_gauss).reshape(S, 1)\n",
        "\n",
        "  for k in k_range:\n",
        "    Phi_mc = F[:, :d]\n",
        "    L = train_td_representation(P, 1000, 1e-2, k)\n",
        "    approx_range_sl.append(())\n",
        "\n",
        "  ax.plot(k_range, approx_range_sl)\n",
        "  ax.plot(k_range, approx_range_td)\n",
        "  ax.legend(['sl' , 'res', 'td'])\n",
        "  ax.set_title(key, size='x-large')\n",
        "  ax.set_xlabel('Number of features', size='x-large') \n",
        "  ax.set_ylabel('Approx error', size='x-large')\n",
        "plt.savefig('all_ones_reward.png')\n",
        "%download_file all_ones_reward.png"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "bellmanvariance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
