import dedal.configs.external_configurables
import dedal.data.builder
import dedal.data.loaders
import dedal.data.nlp
import dedal.data.specs
import dedal.models.aligners
import dedal.models.encoders
import dedal.models.dedal
import dedal.models.nlp
import dedal.smith_waterman
import dedal.train.logger
import dedal.train.losses
import dedal.train.metrics
import dedal.train.training_loop

SEQUENCE_LENGTH = 128
vocabulary.get_default.vocab = %vocabulary.alternative
SEQUENCE_KEY = 'sequence'

TrainingLoop:
  model_cls = @Dedal
  dataset_builder = @builder.DatasetBuilder()
  loss_fn = @MultiTaskLoss()
  optimizer_cls = @tf.keras.optimizers.Adam
  logger_cls = @logger.Logger
  batch_size = 32
  num_steps = 10
  num_eval_steps = 12
  num_steps_per_train_iteration = 5

DatasetBuilder.data_loader = @TFDSLoader()
TFDSLoader.name = 'fake'
TFRecordsLoader.output_sequence_key = %SEQUENCE_KEY
DatasetBuilder.sequence_key = %SEQUENCE_KEY
DatasetBuilder.transformations = [
    @EOS(),
    @CropOrPad(),
    @DynamicLanguageModelMasker()
]
CropOrPad.size = %SEQUENCE_LENGTH
CropOrPad.random = True
DynamicLanguageModelMasker.on = %SEQUENCE_KEY
DynamicLanguageModelMasker.out = [%SEQUENCE_KEY, 'target', 'weights']
DatasetBuilder.labels = @labels/multi_task.Backbone()
labels/multi_task.Backbone.embeddings = [('target', 'weights')]

Dedal:
  encoder_cls = @TransformerEncoder
  aligner_cls = @aligners.SoftAligner
  heads_cls = @model/multi_task.Backbone()

TransformerEncoder:
  emb_dim = 48
  num_layers = 1
  num_heads = 2
  mlp_dim = 384
  max_len = 128
model/multi_task.Backbone.embeddings = [@DensePerTokenOutputHead]

SoftAligner:
  similarity_cls = @PairwiseBilinearDense
  gap_pen_cls = @ConstantGapPenalties
  align_fn = @smith_waterman.perturbed_alignment_score

perturbed_alignment_score:
  noise = 'normal'
  sigma = 1.0
  num_samples = 1
  stop_paths_gradient = True

MultiTaskLoss.losses = @loss/multi_task.Backbone()
loss/multi_task.Backbone.embeddings = [
    @tf.keras.losses.SparseCategoricalCrossentropy()
]
tf.keras.losses.SparseCategoricalCrossentropy.from_logits = True
tf.keras.losses.SparseCategoricalCrossentropy.reduction = 'none'

# Training parameters
tf.keras.optimizers.Adam.learning_rate = 1e-4

# Evaluation
Logger.scalars = @scalars/multi_task.Backbone()
scalars/Backbone.embeddings = [
    [
      @tf.keras.metrics.SparseCategoricalAccuracy,
      @tf.keras.metrics.SparseCategoricalCrossentropy,
    ],
]
tf.keras.metrics.SparseCategoricalCrossentropy.from_logits = True

# Logging
period = 5
Logger.every = %period
Checkpointer.save_every = %period
Checkpointer.max_to_keep = 5
